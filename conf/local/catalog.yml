# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://docs.kedro.org/en/stable/data/data_catalog.html
#
# We support interacting with a variety of data stores including local file systems, cloud, network and HDFS
#
# An example data set definition can look as follows:
#
# The Data Catalog supports being able to reference the same file using two different Dataset implementations
# (transcoding), templating and a way to reuse arguments that are frequently repeated. See more here:
# https://docs.kedro.org/en/stable/data/data_catalog.html


reviews_partition:
  type: partitions.PartitionedDataset
  path: data/01_raw/reviews/
  dataset:
    type: pandas.CSVDataset
  filename_suffix: ".csv"
  metadata:
      description: "Partition of raw CSV file containing reviews' data."

shuttles_partition:
  type: partitions.PartitionedDataset
  path: data/01_raw/shuttles/
  dataset:
    type: pandas.CSVDataset
  filename_suffix: ".csv"
  metadata:
      description: "Partition of raw CSV file containing shuttles' data."

reviews:
  type: pandas.CSVDataset
  filepath: data/01_raw/reviews.csv
  metadata:
    description: "A CSV file containing raw reviews' data."

shuttles:
  type: pandas.ExcelDataset
  filepath: data/01_raw/shuttles.xlsx
  load_args:
    engine: openpyxl
  metadata:
    description: "An Excel file containing raw shuttles' data."

"{namespace}.preprocessed_companies_dagster_partition":
  type: kedro_dagster.DagsterPartitionedDataset
  path: data/02_intermediate/local/{namespace}/preprocessed_companies/
  dataset:
    type: pandas.CSVDataset
  partition:
    type: dagster.StaticPartitionsDefinition
    partition_keys: ["10.csv","20.csv", "30.csv"]
  metadata:
      description: "Dagster partition of raw CSV file containing preprocessed companies' data."

companies_partition:
  type: partitions.PartitionedDataset
  path: data/01_raw/companies/
  dataset:
    type: pandas.CSVDataset
  metadata:
      description: "Partition of raw CSV file containing reviews' data."

"{namespace}.preprocessed_companies_partition":
  type: partitions.PartitionedDataset
  path: data/02_intermediate/local/{namespace}/preprocessed_companies/
  dataset:
    type: pandas.CSVDataset
  metadata:
      description: "Partition of raw CSV file containing preprrocessed companies' data."

"{namespace}.preprocessed_companies":
  type: pandas.ParquetDataset
  filepath: data/02_intermediate/local/{namespace}/preprocessed_companies.pq
  metadata:
    description: "A Parquet file containing preprocessed companies's data for {namespace}."

"{namespace}.is_company_preprocessing_done":
  type: kedro_dagster.DagsterNothingDataset

"{namespace}.preprocessed_shuttles":
  type: pandas.ParquetDataset
  filepath: data/02_intermediate/local/{namespace}/preprocessed_shuttles.pq
  metadata:
    description: "A Parquet file containing preprocessed shuttles's data for {namespace}."

"{namespace}.model_input_table":
  type: pandas.ParquetDataset
  filepath: data/03_primary/local/{namespace}/model_input_table.pq
  metadata:
    description: "Model input table for {namespace}"

"{namespace}.{variant}.regressor":
  type: kedro_mlflow.io.models.MlflowModelTrackingDataset
  flavor: mlflow.sklearn
  artifact_path: "{namespace}/{variant}/regressor"
  metadata:
    description: "Regressor trained for {variant} model for {namespace}."

"{namespace}.{variant}.study":
  type: kedro_datasets_experimental.optuna.StudyDataset
  backend: sqlite
  database: data/06_models/local/{namespace}/{variant}/optuna.db
  study_name: "{namespace}.{variant}"
  load_args:
    sampler:
      class: TPESampler
      n_startup_trials: 2
      n_ei_candidates: 5
    pruner:
      class: NopPruner
  versioned: true
  metadata:
    description: "Tuning study for {variant} model for {namespace}."

"{namespace}.{variant}.tuning_node_done_{i_tuning_node}":
  type: kedro_dagster.DagsterNothingDataset
  metadata:
    description: "Placeholder output for tuning node {i_tuning_node} to enforce order."

"{namespace}.{variant}.study_artifact":
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataset
  dataset:
    type: kedro_datasets_experimental.optuna.StudyDataset
    backend: sqlite
    database: data/06_models/local/{namespace}/{variant}/optuna_sqlite.db
    study_name: "{namespace}.{variant}"
  artifact_path: "{namespace}/{variant}"
  metadata:
    description: "Tuning study MLFlow artifact for {variant} model for {namespace}."
